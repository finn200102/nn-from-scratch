# nn-from-scratch

This is an implementation of an tensor libary with gradient computation with backpropagation using numpy. The mnist.py file trains a 2 layer mlp on the mnist dataset.

## mnist

‚ùØ python mnist.py
Epoch 1/5, Avg Loss: 0.81070434072513097856---time:1855.9657669067383
Epoch 2/5, Avg Loss: 0.41178117451751794498---time:1803.6964662075043
Epoch 3/5, Avg Loss: 0.34065644864929106594---time:1804.5928411483765
Epoch 4/5, Avg Loss: 0.30474960915813698037---time:1775.5145380496979
Epoch 5/5, Avg Loss: 0.28150770088822529846---time:1774.1964979171753
Test Accuracy: 0.9223

## Plans

- Optimizing the performance.
- Adding support for batches
- Implementations of convolutional nns
- Creating proper tests
